{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e00e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import gzip\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Any, List, Sequence, Callable\n",
    "from itertools import islice, zip_longest\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2397666",
   "metadata": {},
   "source": [
    "## Origina data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad35605",
   "metadata": {},
   "source": [
    "###### Download NQ Train and Dev dataset from https://ai.google.com/research/NaturalQuestions/download\n",
    "###### NQ Train: https://storage.cloud.google.com/natural_questions/v1.0-simplified/simplified-nq-train.jsonl.gz\n",
    "###### NQ Dev: https://storage.cloud.google.com/natural_questions/v1.0-simplified/nq-dev-all.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca748ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dev = []\n",
    "\n",
    "with gzip.open(\"v1.0-simplified_nq-dev-all.jsonl.gz\", \"r+\") as f:\n",
    "    for item in jsonlines.Reader(f):\n",
    "        \n",
    "        arr = []\n",
    "        ## question_text\n",
    "        question_text = item['question_text']\n",
    "        arr.append(question_text)\n",
    "\n",
    "        tokens = []\n",
    "        for i in item['document_tokens']:\n",
    "            tokens.append(i['token'])\n",
    "        document_text = ' '.join(tokens)\n",
    "        \n",
    "        ## example_id\n",
    "        example_id = str(item['example_id'])\n",
    "        arr.append(example_id)\n",
    "\n",
    "        # document_text = item['document_text']\n",
    "        ## long_answer\n",
    "        annotation = item['annotations'][0]\n",
    "        has_long_answer = annotation['long_answer']['start_token'] >= 0\n",
    "\n",
    "        long_answers = [\n",
    "            a['long_answer']\n",
    "            for a in item['annotations']\n",
    "            if a['long_answer']['start_token'] >= 0 and has_long_answer\n",
    "        ]\n",
    "        if has_long_answer:\n",
    "            start_token = long_answers[0]['start_token']\n",
    "            end_token = long_answers[0]['end_token']\n",
    "            x = document_text.split(' ')\n",
    "            long_answer = ' '.join(x[start_token:end_token])\n",
    "            long_answer = re.sub('<[^<]+?>', '', long_answer).replace('\\n', '').strip()\n",
    "        arr.append(long_answer) if has_long_answer else arr.append('')\n",
    "\n",
    "        # short_answer\n",
    "        has_short_answer = annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
    "        short_answers = [\n",
    "            a['short_answers']\n",
    "            for a in item['annotations']\n",
    "            if a['short_answers'] and has_short_answer\n",
    "        ]\n",
    "        if has_short_answer and len(annotation['short_answers']) != 0:\n",
    "            sa = []\n",
    "            for i in short_answers[0]:\n",
    "                start_token_s = i['start_token']\n",
    "                end_token_s = i['end_token']\n",
    "                shorta = ' '.join(x[start_token_s:end_token_s])\n",
    "                sa.append(shorta)\n",
    "            short_answer = '|'.join(sa)\n",
    "            short_answer = re.sub('<[^<]+?>', '', short_answer).replace('\\n', '').strip()\n",
    "        arr.append(short_answer) if has_short_answer else arr.append('')\n",
    "\n",
    "        ## title\n",
    "        arr.append(item['document_title'])\n",
    "\n",
    "        ## abs\n",
    "        if document_text.find('<P>') != -1:\n",
    "            abs_start = document_text.index('<P>')\n",
    "            abs_end = document_text.index('</P>')\n",
    "            abs = document_text[abs_start+3:abs_end]\n",
    "        else:\n",
    "            abs = ''\n",
    "        arr.append(abs)\n",
    "\n",
    "        ## content\n",
    "        if document_text.rfind('</Ul>') != -1:\n",
    "            final = document_text.rindex('</Ul>')\n",
    "            document_text = document_text[:final]\n",
    "            if document_text.rfind('</Ul>') != -1:\n",
    "                final = document_text.rindex('</Ul>')\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "            else:\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "        else:\n",
    "            content = document_text[abs_end+4:]\n",
    "            content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            arr.append(content)\n",
    "        doc_tac = item['document_title'] + abs + content\n",
    "        arr.append(doc_tac)\n",
    "        language = 'en'\n",
    "        arr.append(language)\n",
    "        nq_dev.append(arr)\n",
    "\n",
    "nq_dev_df = pd.DataFrame(nq_dev)\n",
    "nq_dev_df.to_csv(r\"nq_dev.tsv\", sep=\"\\t\", mode = 'w', header=None, index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a93d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_train = []\n",
    "with gzip.open(\"v1.0-simplified_simplified-nq-train.jsonl.gz\", \"r+\") as f:\n",
    "    for item in jsonlines.Reader(f):\n",
    "        ## question_text\n",
    "        arr = []\n",
    "        question_text = item['question_text']\n",
    "        arr.append(question_text)\n",
    "\n",
    "        ## example_id\n",
    "        example_id = str(item['example_id'])\n",
    "        arr.append(example_id)\n",
    "        \n",
    "        document_text = item['document_text']\n",
    "        \n",
    "        ## long_answer\n",
    "        annotation = item['annotations'][0]\n",
    "        has_long_answer = annotation['long_answer']['start_token'] >= 0\n",
    "\n",
    "        long_answers = [\n",
    "            a['long_answer']\n",
    "            for a in item['annotations']\n",
    "            if a['long_answer']['start_token'] >= 0 and has_long_answer\n",
    "        ]\n",
    "        if has_long_answer:\n",
    "            start_token = long_answers[0]['start_token']\n",
    "            end_token = long_answers[0]['end_token']\n",
    "            x = document_text.split(' ')\n",
    "            long_answer = ' '.join(x[start_token:end_token])\n",
    "            long_answer = re.sub('<[^<]+?>', '', long_answer).replace('\\n', '').strip()\n",
    "        arr.append(long_answer) if has_long_answer else arr.append('')\n",
    "\n",
    "        # short_answer\n",
    "        has_short_answer = annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
    "        short_answers = [\n",
    "            a['short_answers']\n",
    "            for a in item['annotations']\n",
    "            if a['short_answers'] and has_short_answer\n",
    "        ]\n",
    "        if has_short_answer and len(annotation['short_answers']) != 0:\n",
    "            sa = []\n",
    "            for i in short_answers[0]:\n",
    "                start_token_s = i['start_token']\n",
    "                end_token_s = i['end_token']\n",
    "                shorta = ' '.join(x[start_token_s:end_token_s])\n",
    "                sa.append(shorta)\n",
    "            short_answer = '|'.join(sa)\n",
    "            short_answer = re.sub('<[^<]+?>', '', short_answer).replace('\\n', '').strip()\n",
    "        arr.append(short_answer) if has_short_answer else arr.append('')\n",
    "\n",
    "        ## title\n",
    "        if document_text.find('<H1>') != -1:\n",
    "            title_start = document_text.index('<H1>')\n",
    "            title_end = document_text.index('</H1>')\n",
    "            title = document_text[title_start+4:title_end]\n",
    "        else:\n",
    "            title = ''\n",
    "        arr.append(title)\n",
    "\n",
    "        ## abs\n",
    "        if document_text.find('<P>') != -1:\n",
    "            abs_start = document_text.index('<P>')\n",
    "            abs_end = document_text.index('</P>')\n",
    "            abs = document_text[abs_start+3:abs_end]\n",
    "        else:\n",
    "            abs = ''\n",
    "        arr.append(abs)\n",
    "\n",
    "        ## content\n",
    "        if document_text.rfind('</Ul>') != -1:\n",
    "            final = document_text.rindex('</Ul>')\n",
    "            document_text = document_text[:final]\n",
    "            if document_text.rfind('</Ul>') != -1:\n",
    "                final = document_text.rindex('</Ul>')\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "            else:\n",
    "                content = document_text[abs_end+4:final]\n",
    "                content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "                content = re.sub(' +', ' ', content)\n",
    "                arr.append(content)\n",
    "        else:\n",
    "            content = document_text[abs_end+4:]\n",
    "            content = re.sub('<[^<]+?>', '', content).replace('\\n', '').strip()\n",
    "            content = re.sub(' +', ' ', content)\n",
    "            arr.append(content)\n",
    "\n",
    "        doc_tac = title + abs + content\n",
    "        arr.append(doc_tac)\n",
    "\n",
    "        language = 'en'\n",
    "        arr.append(language)\n",
    "        nq_train.append(arr)\n",
    "\n",
    "nq_train_df = pd.DataFrame(nq_train)\n",
    "nq_train_df.to_csv(r\"nq_train.tsv\", sep=\"\\t\", mode = 'w', header=None, index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping tool\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def lower(x):\n",
    "    text = tokenizer.tokenize(x)\n",
    "    id_ = tokenizer.convert_tokens_to_ids(text)\n",
    "    return tokenizer.decode(id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82baca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc_tac denotes the concatenation of title, abstract and content\n",
    "\n",
    "nq_dev = pd.read_csv('nq_dev.tsv', \\\n",
    "                     names=['query', 'id', 'long_answer', 'short_answer', 'title', 'abstract', 'content', 'doc_tac', 'language'],\\\n",
    "                     header=None, sep='\\t')\n",
    "\n",
    "nq_train = pd.read_csv('nq_train.tsv', \\\n",
    "                       names=['query', 'id', 'long_answer', 'short_answer', 'title', 'abstract', 'content', 'doc_tac', 'language'],\\\n",
    "                       header=None, sep='\\t')\n",
    "\n",
    "nq_dev['title'] = nq_dev['title'].map(lower)\n",
    "nq_train['title'] = nq_train['title'].map(lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c45cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concat train doc and validation doc to obtain full document collection\n",
    "\n",
    "nq_all_doc = nq_train.append(nq_dev)\n",
    "nq_all_doc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicated documents based on titles\n",
    "\n",
    "nq_all_doc.drop_duplicates('title', inplace = True)\n",
    "nq_all_doc.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f51417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The total amount of documents : 109739\n",
    "\n",
    "len(nq_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6146320",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct mapping relation\n",
    "\n",
    "title_doc = {}\n",
    "title_doc_id = {}\n",
    "id_doc = {}\n",
    "ran_id_old_id = {}\n",
    "idx = 0\n",
    "for i in range(len(nq_all_doc)):\n",
    "    title_doc[nq_all_doc['title'][i]] =  nq_all_doc['doc_tac'][i]\n",
    "    title_doc_id[nq_all_doc['title'][i]] = idx\n",
    "    id_doc[idx] = nq_all_doc['doc_tac'][i]\n",
    "    ran_id_old_id[idx] = nq_all_doc['id'][i]\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a461690",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct Document Content File\n",
    "\n",
    "train_file = open(\"NQ_doc_content.tsv\", 'w') \n",
    "\n",
    "for docid in id_doc.keys():\n",
    "    train_file.write('\\t'.join([str(docid), '', '', id_doc[docid], '', '', 'en']) + '\\n')\n",
    "    train_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed730f5",
   "metadata": {},
   "source": [
    "## Generate BERT embeddings for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute the following command to get bert embedding pkl file\n",
    "## Use 4 GPU\n",
    "!./bert/bert_NQ.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concat bert embedding\n",
    "output_bert_base_tensor_nq_qg = []\n",
    "output_bert_base_id_tensor_nq_qg = []\n",
    "for num in trange(4):\n",
    "    with open(f'bert/pkl/NQ_output_tensor_512_content_{num}.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    output_bert_base_tensor_nq_qg.extend(data)\n",
    "\n",
    "    with open(f'bert/pkl/NQ_output_tensor_512_content_{num}_id.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    output_bert_base_id_tensor_nq_qg.extend(data)\n",
    "\n",
    "train_file = open(f\"bert/NQ_doc_content_embedding_bert_512.tsv\", 'w') \n",
    "\n",
    "for idx, doc_tensor in enumerate(output_bert_base_tensor_nq_qg):\n",
    "    embedding = '|'.join([str(elem) for elem in doc_tensor])\n",
    "    train_file.write('\\t'.join([str(output_bert_base_id_tensor_nq_qg[idx]), '', '', '', '', '', 'en', embedding]) + '\\n')\n",
    "    train_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff16d74",
   "metadata": {},
   "source": [
    "## Apply Hierarchical K-Means on it to generate semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f186a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute the following command to get kmeans id of the documents\n",
    "!./kmeans/kmeans_NQ.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e22ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kmeans/IDMapping_NQ_bert_512_k30_c30_seed_7.pkl', 'rb') as f:\n",
    "    kmeans_nq_doc_dict = pickle.load(f)\n",
    "## random id : newid\n",
    "new_kmeans_nq_doc_dict_512 = {}\n",
    "for old_docid in kmeans_nq_doc_dict.keys():\n",
    "    new_kmeans_nq_doc_dict_512[str(old_docid)] = '-'.join(str(elem) for elem in kmeans_nq_doc_dict[old_docid])\n",
    "    \n",
    "new_kmeans_nq_doc_dict_512_int_key = {}\n",
    "for key in new_kmeans_nq_doc_dict_512:\n",
    "    new_kmeans_nq_doc_dict_512_int_key[int(key)] = new_kmeans_nq_doc_dict_512[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78043201",
   "metadata": {},
   "source": [
    "## Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48d0fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Execute the following command to generate queries for the documents\n",
    "## Use 4 GPU\n",
    "!./qg/NQ_qg.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469428f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## merge parallel results\n",
    "output_bert_base_tensor_nq_qg = []\n",
    "output_bert_base_id_tensor_nq_qg = []\n",
    "for num in trange(4):\n",
    "    with open(f'qg/pkl/NQ_output_tensor_512_content_64_15_{num}.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    output_bert_base_tensor_nq_qg.extend(data)\n",
    "\n",
    "    with open(f'qg/pkl/NQ_output_tensor_512_content_64_15_{num}_id.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    output_bert_base_id_tensor_nq_qg.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d972c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qg_dict = {}\n",
    "for i in trange(len(output_bert_base_tensor_nq_qg)):\n",
    "    if(output_bert_base_id_tensor_nq_qg[i] not in qg_dict):\n",
    "        qg_dict[output_bert_base_id_tensor_nq_qg[i]] = [output_bert_base_tensor_nq_qg[i]]\n",
    "    else:\n",
    "        qg_dict[output_bert_base_id_tensor_nq_qg[i]].append(output_bert_base_tensor_nq_qg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0afd5f",
   "metadata": {},
   "source": [
    "## Genarate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9681d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nq_512_qg20.tsv\n",
    "QG_NUM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5d975",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qg_file = open(\"NQ_512_qg.tsv\", 'w') \n",
    "\n",
    "for queryid in tqdm(qg_dict):\n",
    "    for query in qg_dict[queryid][:QG_NUM]:\n",
    "        qg_file.write('\\t'.join([query, str(ran_id_old_id[int(queryid)]), queryid, new_kmeans_nq_doc_dict_512[queryid]]) + '\\n')\n",
    "        qg_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kmeans_nq_doc_dict_512_int_key = {}\n",
    "for key in new_kmeans_nq_doc_dict_512:\n",
    "    new_kmeans_nq_doc_dict_512_int_key[int(key)] = new_kmeans_nq_doc_dict_512[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace Original IDs with Semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nq_train_doc_newid.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120bc7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nq_train['randomid'] = nq_train['title'].map(title_doc_id)\n",
    "nq_train['id_512'] = nq_train['randomid'].map(new_kmeans_nq_doc_dict_512_int_key)\n",
    "\n",
    "nq_train_ = nq_train.loc[:, ['query', 'id', 'randomid', 'id_512']]  \n",
    "nq_train_.to_csv('nq_train_doc_newid.tsv', sep='\\t', header=None, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nq_dev_doc_newid.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dev['randomid'] = nq_dev['title'].map(title_doc_id)\n",
    "nq_dev['id_512'] = nq_dev['randomid'].map(new_kmeans_nq_doc_dict_512_int_key)\n",
    "\n",
    "\n",
    "nq_dev_ = nq_dev.loc[:, ['query', 'id', 'randomid', 'id_512']]  \n",
    "nq_dev_.to_csv('nq_dev_doc_newid.tsv', sep='\\t', header=None, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f79bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## title+abs oldid newid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nq_title_abs.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_all_doc_non_duplicate = nq_train.append(nq_dev)\n",
    "nq_all_doc_non_duplicate.reset_index(inplace = True)\n",
    "\n",
    "nq_all_doc_non_duplicate['id_512'] = nq_all_doc_non_duplicate['randomid'].map(new_kmeans_nq_doc_dict_512_int_key)\n",
    "\n",
    "nq_all_doc_non_duplicate['ta'] = nq_all_doc_non_duplicate['title'] + ' ' + nq_all_doc_non_duplicate['abstract']\n",
    "\n",
    "nq_all_doc_non_duplicate = nq_all_doc_non_duplicate.loc[:, ['ta', 'id', 'randomid','id_512']]  \n",
    "nq_all_doc_non_duplicate.to_csv('nq_title_abs.tsv', sep='\\t', header=None, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8356d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all_doc_aug_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NQ_doc_aug.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee893b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryid_oldid_dict = {}\n",
    "bertid_oldid_dict = {}\n",
    "map_file = \"./nq_title_abs.tsv\"\n",
    "with open(map_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        query, queryid, oldid, bert_k30_c30 = line.split(\"\\t\")\n",
    "        queryid_oldid_dict[oldid] = queryid\n",
    "        bertid_oldid_dict[oldid] = bert_k30_c30\n",
    "\n",
    "train_file = \"./NQ_doc_content.tsv\"\n",
    "doc_aug_file = open(f\"./NQ_doc_aug.tsv\", 'w') \n",
    "with open(train_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        docid, _, _, content, _, _, _ = line.split(\"\\t\")\n",
    "        content = content.split(' ')\n",
    "        add_num = max(0, len(content)-3000) / 3000\n",
    "        for i in range(10+int(add_num)):\n",
    "            begin = random.randrange(0, len(content))\n",
    "            # if begin >= (len(content)-64):\n",
    "            #     begin = max(0, len(content)-64)\n",
    "            end = begin + 64 if len(content) > begin + 64 else len(content)\n",
    "            doc_aug = content[begin:end]\n",
    "            doc_aug = ' '.join(doc_aug)\n",
    "            queryid = queryid_oldid_dict[docid]\n",
    "            bert_k30_c30 = bertid_oldid_dict[docid]\n",
    "            # doc_aug_file.write('\\t'.join([doc_aug, str(queryid), str(docid), str(bert_k30_c30)]) + '\\n')\n",
    "            doc_aug_file.write('\\t'.join([doc_aug, str(queryid), str(docid), str(bert_k30_c30)]))\n",
    "            doc_aug_file.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss",
   "language": "python",
   "name": "faiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
